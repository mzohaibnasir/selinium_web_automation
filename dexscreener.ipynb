{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By  # search for elements\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# thee will allow us to wait for presence of the element\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.binary_location = \"./chrome-headless-shell-linux64/chrome-headless-shell\"\n",
    "# chrome_options.add_argument(\"--headless\")  # Ensure headless mode\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")  # Sets window size\n",
    "\n",
    "# Set up the service\n",
    "service = Service()\n",
    "\n",
    "# Initialize the driver with the configured options\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Now you can use the driver\n",
    "driver\n",
    "\n",
    "driver.get(\"https://dexscreener.com/\")\n",
    "\n",
    "\n",
    "driver.get(\"https://dexscreener.com/\")\n",
    "input(\"Please solve the CAPTCHA and press Enter to continue...\")\n",
    "\n",
    "\n",
    "# Wait for the checkbox to be present\n",
    "try:\n",
    "    # Use the complete XPath to locate the checkbox\n",
    "    checkbox = WebDriverWait(driver, 40).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (By.XPATH, \"/html/body//div/div/div[1]/div/label/input\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Check if the checkbox is not already selected\n",
    "    if not checkbox.is_selected():\n",
    "        checkbox.click()  # Click to check the checkbox\n",
    "\n",
    "    print(\"Checkbox is checked!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    time.sleep(5)  # Pause to observe the action (optional)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element not found. The website structure might have changed.\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_dex_data():\n",
    "    # Create a scraper that bypasses cloudflare\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "\n",
    "    # Get the webpage\n",
    "    response = scraper.get(\"https://dexscreener.com/\")\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the specific element using the CSS selector\n",
    "    target_element = soup.select_one(\n",
    "        \"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\"\n",
    "    )\n",
    "\n",
    "    if target_element:\n",
    "        return target_element.text\n",
    "    else:\n",
    "        return \"Element not found. The website structure might have changed.\"\n",
    "\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    result = scrape_dex_data()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta name=\"robots\" content=\"noindex,nofollow\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><style>*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width <= 720px){.main-content{margin-top:4rem}}.h2{font-size:1.5rem;font-weight:500;line-height:2.25rem}@media (width <= 720px){.h2{font-size:1.25rem;line-height:1.5rem}}#challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+);background-repeat:no-repeat;background-size:contain;padding-left:34px}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}}</style><meta http-equiv=\"refresh\" content=\"390\"></head><body class=\"no-js\"><div class=\"main-wrapper\" role=\"main\"><div class=\"main-content\"><noscript><div class=\"h2\"><span id=\"challenge-error-text\">Enable JavaScript and cookies to continue</span></div></noscript></div></div><script>(function(){window._cf_chl_opt={cvId: '3',cZone: \"dexscreener.com\",cType: 'managed',cRay: '901b8bad9df2e1bc',cH: 'yQYG7kjbG7LyvWkoFAJRVvZnFFLi0B4wNpPbppA6gdA-1736835893-1.2.1.1-g4nHKxR_foQsd6D9YEG2ClfQ2Mp7Y_4kgP2ER7u5ZFNNclIWVfQDbQv3Imp58Vzd',cUPMDTk: \"\\/?__cf_chl_tk=G7emuEY71VyE3PC1zy17ACZB_I1A8ubh6EbDFDjsQc4-1736835893-1.0.1.1-LckeMGgozDS3xS4hrNu7Gx.36_wF7snzkXmgG2psxfw\",cFPWv: 'b',cITimeS: '1736835893',cTTimeMs: '1000',cMTimeMs: '390000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/?__cf_chl_f_tk=G7emuEY71VyE3PC1zy17ACZB_I1A8ubh6EbDFDjsQc4-1736835893-1.0.1.1-LckeMGgozDS3xS4hrNu7Gx.36_wF7snzkXmgG2psxfw\",md: \"2ahLCzm6ybJbIsmYpBICIzuzUcEQJR_81SPNn7j6BEw-1736835893-1.2.1.1-jw4lRidAIw3hQfPGoE9RUH.YK_JkWa1gK71BAyzgH389zllWYC5bIRs8xaJTjRmJuxyUZfwtZlwt90KZY3xaoq5FVz_x4VeQCj0uapeMB3zkqNMb9ULOsac23cWjp36TyEBZmsVnQKKzOfF0C6dRGE1ZX2mHdZG8FHcOC3Zq8QjNL65LcmiIr93oT2LAyygTSqqlD3ERsyresK.QAmGokf81QwKPLuWe99tO1IQdyyQjt8VFaQxD3sBwys6jCWAhb4eKSkrMatKoJEa3by8ev4OIlun_y7nWzHJeBnU5nsshx69CsUWBh4rErUy0CwXeBBM.MAGY.mUBTWyBvdUq8CRWXCRqhqPdomMFzjU7rd.BNpAf1s1dXk4wb2dJ4EQFv7UqRx4.kSnTmavCiDF7VEc_AY.FWkBmKpYTg15BA.N8pD7xN_jnb6FMmJtORpIgRpw1z3bnhB8DcDw1xYyAlrUvrPb95qGuKv1IWdnkNj.7K1C2cMrKeut.B0c1XJVNani0OZxK7MxX0SK1iJAUEAQWYyhmEGqtOJMxViOaIt9nERKLlk1aKsn3azw.pr2N5QVI0yxW6Q8R9lnWrOUJ_LHb7wgQmtYJnFGvJko5UWQ..qB_S5Y50.GZpvB24u2gdboQEQHG.JrgWIQFjM5vJe6INgjvB.nsLgTiTd8skFcbnsiM8Ngr7a.F9tKdgqSeU9GhklIqRec9CnIvYrEdXxnBytH4PBSzVDOzkz590HGd07H1hZnx31V0CdWhzR_Imo9Vn9qj7d.JrdaK.mw8p7d7Bz6H1HfFjY9nd09B3bzCuV4pD5Ehka09xEhaZlBhYcpYfZiCfPgSlgtsMHFttyFFt2NiNzYlXVmF8CUccOZmLLqCZAKdgaQHGbH.r3fdPsnI.eK5HNG7ivEQucpUSEH2SEbU7VWghXlzfu17..uJoxFmOh2Gg7lUaVIg2LNwBBjFCHDrkNyAq1VHByJfz9hOqsFgJqrCz9uYR5907Vtzr0mw56bBbvB3Q4AEP6puDqS.5MaM8OjCCwPXZ2cqrbeijE.qNzH9Ild_VN_YR_KosgGOpBhMXBXlRPFbTTKEoHgEDGPMI9NltYh6fKUFFsoB6d0qWpJofUC4KaFp76400EzOf1egn25r5EEjDdfgG7VQTme_W_aaKMovf8pz_lEgmLVfp7XcvGbIJU2jiBGrYSWRYKp0lVSA7WldQCnnr.R1EGaUlqt7EbLYD2Po2H1rOk67.exJee9woutcOZx1JZ3AgqqhiFWAZzlkSvQnwHE6PVRE4SbEFX7_mPCjxOuc3yHGzpHmgONc85__OEpjC1dLbuxsWMHxx1Y947oR07AJVjz1t7Bst1OfFEi._san5cIOh19Y0801HAMbUaT9yQ8pYPJNrF28xqIAEoSr2Pl5JAo.spVkHX8R11U0O8lVSMuMb2TyV51S9T5Imik7IUGncu4vx01gBRfpnPSJ1udJx0aems.YXTQkYJY5DM4qq2Z83Jla.ujh2SExnDSjgecWRI6swjpdWgQPa_6RfnzYYxyfdeYL_XVryV8rr934X7LK.oQB2YRskuklE.0iILcidMtKTws_hYYWmsAZb.4MRj7q7gftUzy0yntN2KeVKRBHdbC0fxU8wOG_S9XFLnilVoQNdGx4uL_QJ4wSUlzcH4L8PxADKsthrpoQWZuqxMDyyyc8s6Gn8PX.rjcmwKEQKX0B2Atm7yUw7Whgj2zds5uLsgiyMTrageyF4gPe6AKce0vkM6zyqNJ8Ucl2MVd2ANnTXmFtk0iUnrLxZh5oOPgdRc036pbswMJ_SFD_rM863FrbzSPy0hKOXN.5MpiUqCsf5KHpENyWW2k270UOI5METZaT6LdMUthBty0g.3Twkit0lpQh0AFNuIo_oqlCGDsb30JG0WlrIPuJbPDhPxrrVvedgz4KEr239ZIpLuhoG7UHgCfpwksTr3bFFVslwExUpol5oK0Bu1FlxuajqYBbpkPaxDIRh8Vs0ad46gTyJSDv5Aw2p7l9tRNkQD4QIGr79VHQ6KJ37Dnzm6eTonGPSNHOxuiNiLW0b1X0kZfCi5exBBmQUFkT3RSksZBorY_uuD68Vv1OM8sHkm0UeUVgRb5yUt6zOepUHBTlhVT2ZBQr9zMJudG2d_vTHGCWNFafQC__sCMDvwBn0lWYe5UJ0nL6xMHHtn1rVquR5XkjlsOrN7iiEP432Brk2o6oMYBWeqNvB4K0yjIyNQR40VmqEvSmK6uYjpRcMh3f.dJDbzs_XdSsQOy4br06nDTkodOaj_otaB4Xj3fxFLsf8fdAa3E4ALl0hiBP8J1c5ehNSxLv5cuGKJN8ZlbKZCs5tZIHkSq025B68QCdvKTkpYIXUgma5S9AR93yug\",mdrd: \"7_yHm0SKdU5AbJTCWMpoGHmTa9UpXJ2HP1RhW9jhkF4-1736835893-1.2.1.1-RQyyufN4ZsTlxYUulU.ZIbV6LrNnoVCM5H7G9TpdjShQ.SmXCE4FCV6kfXh27qtmht7hSV17snQp2euY1Z3I8fdA7X3wlkNMHn9HsWvSsZ6FSdVcTLiNeCCXzzcW2axHhEGM1ekNva1hJAbmM3iZQ9IKdy5Bn7vfFtF7gTz8lChQD9QhU0hNSDXdF9SgsnNamifSDjHeOQ57EL6AOFGfS4UxYBjEVOYZlgPy5JtHI..bC9xSoPOC5XZsAE6Wk.fkfoQ8EA0rGuW5l7ThY.NEoZGItr8nm9zSSVmD50_vWRFvOHnuOWX4bjJqo4V3PTSlX4IRzMvBk_17wILV39AifZKk0llE159RWEh6CHYsaETSUCfCxuTpibtF9KiDO4aDWzZTMc1B4Cpo5ferfnuj46pMTfWuCFjxTuy73pqRXGlrzmCqjAMbNW4w5zfaK17zwoCuPQtAe4xlOZ0gXd_ylzj6H8dQBKl.YE3wknc1YpMyWkie08AQLo2oAvcpZK_JzNtdHEzNdaw8RUqcbM.g0vMQaj2syWMm5sw1dgXI1ADTjkG7bhMBfBe9UWc1nLXCYCXzRTYXF7PdvAcua8vcKOReaLKjj75Ik9_dZehaBieJhW6OXJDsPPnScAysZTmeeEVd1rNQJR41jF3yPiHUYmcoAbjzYyjdRwvhX62zWmHWP0wL5SDvy7IyTV1fAwlHnKJ2DLtuOFPCW8gAYfKHJzANlIcWa5knpeRvnmdS6oSTB5.YFTRC40bHbuZOfRfcVSKi2q4OjmspssNGSt.COvKE9HNd198.FWyHcBzT4c2DbJ.hy9WR.1xtxwkTTYPevi2V4nwJi.xkWF2UCB1YMyWgGlXO0o3pzy53jkKAnh6bfPCESXsZ1lBFunncMxNXmU6xlIKN8mUGikKEvBnbblG0c.HhhXYu5skDiJfNAcIjidWj7qsL_ypU8Adclmp.UxahXiKkUi__SAR5iKiad8l9n8j0Otiv4T140JSntLBnHQbu.kWg.5eqIpEajB6PGZRJF.fyCYPDNigVBwiFfpZIqhxuQPAM.nQQfEZJ5tUI9MZ12q3.gsIYVb2Xmk1CUiBT817zN7MDYkgNot6euTdviBqy0p2LVNG2jGaI1RoxxozAbUQH1vTX.ZAHFBkQcgfLrQsM9Qu0Mrtc9dvwPg4eUOPKVYjXgWeVTKSPSpnIchkq4qtmzKRfgMbU3.VfRMDQxAqElVNKodpVrPkWKgjl5yQCr6H3bWmj_0rD1MnDIt41qm3015r8HrU.Sazou2xha.N38nZ_elOQungmQdON_FZFXuj6XeC4Sd2_7NrAhnnmTEgU_qWbxrC11SgFOMOYipRtNgeXmVbnU54vQi6I3SgFA6mJeGerJzU0oCciEeHmwqDkH0aWhTKmNzP9mS1B8et3S6CZgL.qZpodP3IHmvr.Ktd6ETpK6.tdlBkEbIg6Vj_OR3RN1Z49CWYhtz12NGzcbna2j5PtxADCLcvKdtVhkw4u5Wo6A6dLcPGaw7.ezQ2BGQ7E21EWEoRVkU_q.nZYCSa__cEA5AkxeJgYAJB.kr4LI7PBUyiO8iGtuCuNVO.osUL4j7yvMzMQi2GPxC90pUDA84Rh_xoYE6wCrIBXIqFbVby1A8L_PmOFckmuLPufKl0kxo3xNl9u_zPRMPM1eNrYfIYDfS7VIM0fNcFAXLIhx2bFmie20Izezhg.5sRIWenCsHmpcCyplg5B1AGG5T33YxR_8dU7nrkjVGQ2gP.yIxWGCzp617g6jCDZlQJ4TfjccrlRggrezmJiisQi1jl1mltp3TLzaPoeSJqk47U2i2qwI45Lvj1tGqsJvgE5tVLWt3eaC4Ewrrf1s411MlMxKGPs5Tn50qAbWL3.PHs.PzDHJa5hjhbHP_isB90LrBG_0l4W7b37NOqzBammtXjriNxGUbqa5O1RsDUSI0w4ljboPk1gcwf7atN3_kwibUu5_E8f_n4DI9aB1OmdvOyYDqecKeHFEqB_MiVhGWWP1tAZiirYCgPqTG4xOqVGeSrazBr0CW.WGzeJSOh3luR5_5WGzpJq_5.N8AAD9iA2FPRPZ7AQ4b7DJGo2Ml19Xt3L._YKqZs5XGWrB9xdjPdF22yiDZNFR39KwOqqt24IXLA85zqRTXg18q0_5suj4O8vHHsU7FSbNEKVlx2tf5YoSQmP8rftrDI7rFYtVbGKQ.fjW1NNCnFJt9gcruQfDwuwva_vqOTEASUmVl0QKq03Ru5zhh6GPicr5c7OlLQjzUJUaavmtRnstO0hQNIPAKbOveqh0BOduUFvBnsrdvN4jB7CByDawQ\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=901b8bad9df2e1bc';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/?__cf_chl_rt_tk=G7emuEY71VyE3PC1zy17ACZB_I1A8ubh6EbDFDjsQc4-1736835893-1.0.1.1-LckeMGgozDS3xS4hrNu7Gx.36_wF7snzkXmgG2psxfw\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());</script></body></html>\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "response = scraper.get(\"https://dexscreener.com/\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cloudscraper\n",
      "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from cloudscraper) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.9.2 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from cloudscraper) (2.32.3)\n",
      "Collecting requests-toolbelt>=0.9.1 (from cloudscraper)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from requests>=2.9.2->cloudscraper) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from requests>=2.9.2->cloudscraper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from requests>=2.9.2->cloudscraper) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from requests>=2.9.2->cloudscraper) (2024.8.30)\n",
      "Downloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Installing collected packages: requests-toolbelt, cloudscraper\n",
      "Successfully installed cloudscraper-1.2.71 requests-toolbelt-1.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<input type=\"checkbox\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specified element was not found.\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create a Cloudflare scraper session\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "# Get the webpage content\n",
    "response = scraper.get(\"https://dexscreener.com/\")\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract the specific div element\n",
    "target_div = soup.find(\"div\", class_=\"ds-dex-table ds-dex-table-top\")\n",
    "\n",
    "# Check if the element is found and print its content\n",
    "if target_div:\n",
    "    print(\"Extracted Content:\")\n",
    "    print(target_div.prettify())  # Nicely formatted HTML\n",
    "else:\n",
    "    print(\"The specified element was not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=131.0.6778.69)\n",
      "Stacktrace:\n",
      "#0 0x55bd4c0cc1fa <unknown>\n",
      "#1 0x55bd4bbdc810 <unknown>\n",
      "#2 0x55bd4bbb248f <unknown>\n",
      "#3 0x55bd4bc579bd <unknown>\n",
      "#4 0x55bd4bc6d9bc <unknown>\n",
      "#5 0x55bd4bc4f323 <unknown>\n",
      "#6 0x55bd4bc1dde0 <unknown>\n",
      "#7 0x55bd4bc1edbe <unknown>\n",
      "#8 0x55bd4c09812b <unknown>\n",
      "#9 0x55bd4c09c0c7 <unknown>\n",
      "#10 0x55bd4c0856cc <unknown>\n",
      "#11 0x55bd4c09cc47 <unknown>\n",
      "#12 0x55bd4c06a67f <unknown>\n",
      "#13 0x55bd4c0bb288 <unknown>\n",
      "#14 0x55bd4c0bb450 <unknown>\n",
      "#15 0x55bd4c0cb076 <unknown>\n",
      "#16 0x7dfe49894ac3 <unknown>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "# Set up the service\n",
    "service = Service()\n",
    "\n",
    "# Initialize the driver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Open the website\n",
    "driver.get(\"https://dexscreener.com/\")\n",
    "\n",
    "try:\n",
    "    # Wait for the dynamic content to load\n",
    "    target_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (\n",
    "                By.CLASS_NAME,\n",
    "                \"ds-dex-table ds-dex-table-top\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"Extracted Content:\")\n",
    "    print(\n",
    "        target_element.get_attribute(\"outerHTML\")\n",
    "    )  # Get the HTML content of the element\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch the page. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create a Cloudflare scraper session\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "# Fetch the webpage content\n",
    "response = scraper.get(\"https://dexscreener.com/\")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find all elements with the target class\n",
    "    elements = soup.find_all(\"div\", class_=\"ds-dex-table-row ds-dex-table-row-top\")\n",
    "\n",
    "    # Check if any elements are found\n",
    "    if elements:\n",
    "        print(f\"Found {len(elements)} elements:\")\n",
    "        for idx, element in enumerate(elements, start=1):\n",
    "            print(f\"\\nElement {idx}:\")\n",
    "            print(element.prettify())  # Nicely formatted HTML\n",
    "    else:\n",
    "        print(\"No elements with the specified class were found.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting undetected-chromedriver\n",
      "  Downloading undetected-chromedriver-3.5.5.tar.gz (65 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: selenium in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (4.27.1)\n",
      "Requirement already satisfied: requests in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from undetected-chromedriver) (2.32.3)\n",
      "Collecting websockets (from undetected-chromedriver)\n",
      "  Downloading websockets-14.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from selenium) (0.28.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from requests->undetected-chromedriver) (3.4.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/zohaib/anaconda3/envs/torch312/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading websockets-14.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
      "Building wheels for collected packages: undetected-chromedriver\n",
      "  Building wheel for undetected-chromedriver (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for undetected-chromedriver: filename=undetected_chromedriver-3.5.5-py3-none-any.whl size=47047 sha256=5ed2f12a384c251fe83ac9fa4aa460477d668109fcdf4bce58c5ababe7d92943\n",
      "  Stored in directory: /home/zohaib/.cache/pip/wheels/c4/f1/aa/9de6cf276210554d91e9c0526864563e850a428c5e76da4914\n",
      "Successfully built undetected-chromedriver\n",
      "Installing collected packages: websockets, undetected-chromedriver\n",
      "Successfully installed undetected-chromedriver-3.5.5 websockets-14.1\n"
     ]
    }
   ],
   "source": [
    "# ! pip install undetected-chromedriver selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing website...\n",
      "An error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Failed to extract content\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "\n",
    "def scrape_dex_data():\n",
    "    try:\n",
    "        # Create an undetected Chrome instance\n",
    "        options = uc.ChromeOptions()\n",
    "        # options.add_argument(\"--headless\")  # Run in headless mode (optional)\n",
    "        options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "        driver = uc.Chrome(options=options)\n",
    "\n",
    "        # Set an implicit wait time\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        print(\"Accessing website...\")\n",
    "        driver.get(\"https://dexscreener.com/\")\n",
    "\n",
    "        # Wait for the element to be present (up to 20 seconds)\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        target_element = wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Give additional time for dynamic content to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Get the element's content\n",
    "        content = target_element.text\n",
    "\n",
    "        return content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = scrape_dex_data()\n",
    "    if result:\n",
    "        print(\"Extracted content:\")\n",
    "        print(result)\n",
    "    else:\n",
    "        print(\"Failed to extract content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxies\n",
    "\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from itertools import cycle\n",
    "import requests\n",
    "\n",
    "\n",
    "class ProxyRotator:\n",
    "    def __init__(self, proxy_list=None):\n",
    "        # Initialize with your proxy list or get free proxies\n",
    "        self.proxies = proxy_list if proxy_list else self.get_free_proxies()\n",
    "        self.proxy_pool = cycle(self.proxies)\n",
    "        self.current_proxy = None\n",
    "        self.calls_with_current_proxy = 0\n",
    "        self.max_calls_per_proxy = 3\n",
    "\n",
    "    def get_free_proxies(self):\n",
    "        \"\"\"Get a list of free proxies (for demonstration - replace with your paid proxy service)\"\"\"\n",
    "        return [\n",
    "            # Add your proxy list here in format:\n",
    "            # \"ip:port\"\n",
    "            # Example:\n",
    "            # \"192.168.1.1:8080\",\n",
    "            # \"192.168.1.2:8080\"\n",
    "        ]\n",
    "\n",
    "    def get_next_proxy(self):\n",
    "        \"\"\"Get the next proxy from the pool\"\"\"\n",
    "        if self.calls_with_current_proxy >= self.max_calls_per_proxy:\n",
    "            self.current_proxy = next(self.proxy_pool)\n",
    "            self.calls_with_current_proxy = 0\n",
    "\n",
    "        if self.current_proxy is None:\n",
    "            self.current_proxy = next(self.proxy_pool)\n",
    "\n",
    "        self.calls_with_current_proxy += 1\n",
    "        return self.current_proxy\n",
    "\n",
    "\n",
    "def scrape_dex_data(proxy_rotator):\n",
    "    try:\n",
    "        # Get the next proxy\n",
    "        current_proxy = proxy_rotator.get_next_proxy()\n",
    "\n",
    "        # Create Chrome options\n",
    "        options = uc.ChromeOptions()\n",
    "        options.add_argument(f\"--proxy-server={current_proxy}\")\n",
    "        options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "        # Additional options to help avoid detection\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-infobars\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-browser-side-navigation\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "        print(f\"Using proxy: {current_proxy}\")\n",
    "        driver = uc.Chrome(options=options)\n",
    "\n",
    "        # Set an implicit wait time\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        print(\"Accessing website...\")\n",
    "        driver.get(\"https://dexscreener.com/\")\n",
    "\n",
    "        # Wait for the element to be present\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        target_element = wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add random delay to mimic human behavior\n",
    "        time.sleep(random.uniform(3, 7))\n",
    "\n",
    "        # Get the element's content\n",
    "        content = target_element.text\n",
    "\n",
    "        return content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with proxy {current_proxy}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the proxy rotator with your proxy list\n",
    "    proxy_list = [\n",
    "        # Add your proxies here\n",
    "        # \"ip1:port1\",\n",
    "        # \"ip2:port2\",\n",
    "        # etc.\n",
    "    ]\n",
    "\n",
    "    proxy_rotator = ProxyRotator(proxy_list)\n",
    "\n",
    "    # Number of scraping attempts\n",
    "    max_attempts = 10\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"\\nAttempt {attempt + 1}/{max_attempts}\")\n",
    "        result = scrape_dex_data(proxy_rotator)\n",
    "\n",
    "        if result:\n",
    "            print(\"Successfully extracted content:\")\n",
    "            print(result)\n",
    "            # Add delay between requests\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "        else:\n",
    "            print(\"Failed to extract content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 105\u001b[0m\n\u001b[1;32m    100\u001b[0m         driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Initialize the proxy rotator with free proxies\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     proxy_rotator \u001b[38;5;241m=\u001b[39m \u001b[43mProxyRotator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Number of scraping attempts\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     max_attempts \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mProxyRotator.__init__\u001b[0;34m(self, proxy_list)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, proxy_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Initialize with your proxy list or fetch free proxies\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxies \u001b[38;5;241m=\u001b[39m proxy_list \u001b[38;5;28;01mif\u001b[39;00m proxy_list \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_free_proxies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy_pool \u001b[38;5;241m=\u001b[39m cycle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxies)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mProxyRotator.get_free_proxies\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable tbody tr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     28\u001b[0m     cells \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m---> 30\u001b[0m         \u001b[43mcells\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melite proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m cells[\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     ):\n\u001b[1;32m     33\u001b[0m         proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcells[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcells[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         proxies\u001b[38;5;241m.\u001b[39mappend(proxy)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "from itertools import cycle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class ProxyRotator:\n",
    "    def __init__(self, proxy_list=None):\n",
    "        # Initialize with your proxy list or fetch free proxies\n",
    "        self.proxies = proxy_list if proxy_list else self.get_free_proxies()\n",
    "        self.proxy_pool = cycle(self.proxies)\n",
    "        self.current_proxy = None\n",
    "        self.calls_with_current_proxy = 0\n",
    "        self.max_calls_per_proxy = 3\n",
    "\n",
    "    def get_free_proxies(self):\n",
    "        \"\"\"Scrape free proxies from a public proxy website\"\"\"\n",
    "        url = \"https://free-proxy-list.net/\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        proxies = []\n",
    "        for row in soup.select(\"table tbody tr\"):\n",
    "            cells = row.find_all(\"td\")\n",
    "            if (\n",
    "                cells[4].text.strip() == \"elite proxy\"\n",
    "                and cells[6].text.strip() == \"yes\"\n",
    "            ):\n",
    "                proxy = f\"{cells[0].text.strip()}:{cells[1].text.strip()}\"\n",
    "                proxies.append(proxy)\n",
    "        return proxies\n",
    "\n",
    "    def get_next_proxy(self):\n",
    "        \"\"\"Get the next proxy from the pool\"\"\"\n",
    "        if self.calls_with_current_proxy >= self.max_calls_per_proxy:\n",
    "            self.current_proxy = next(self.proxy_pool)\n",
    "            self.calls_with_current_proxy = 0\n",
    "\n",
    "        if self.current_proxy is None:\n",
    "            self.current_proxy = next(self.proxy_pool)\n",
    "\n",
    "        self.calls_with_current_proxy += 1\n",
    "        return self.current_proxy\n",
    "\n",
    "\n",
    "def scrape_dex_data(proxy_rotator):\n",
    "    try:\n",
    "        # Get the next proxy\n",
    "        current_proxy = proxy_rotator.get_next_proxy()\n",
    "\n",
    "        # Create Chrome options\n",
    "        options = uc.ChromeOptions()\n",
    "        options.add_argument(f\"--proxy-server=http://{current_proxy}\")\n",
    "        options.add_argument(\"--window-size=1920,1080\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-infobars\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-browser-side-navigation\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "        print(f\"Using proxy: {current_proxy}\")\n",
    "        driver = uc.Chrome(options=options)\n",
    "\n",
    "        # Set an implicit wait time\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        print(\"Accessing website...\")\n",
    "        driver.get(\"https://dexscreener.com/\")\n",
    "\n",
    "        # Wait for the element to be present\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        target_element = wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add random delay to mimic human behavior\n",
    "        time.sleep(random.uniform(3, 7))\n",
    "\n",
    "        # Get the element's content\n",
    "        content = target_element.text\n",
    "\n",
    "        return content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with proxy {current_proxy}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the proxy rotator with free proxies\n",
    "    proxy_rotator = ProxyRotator()\n",
    "\n",
    "    # Number of scraping attempts\n",
    "    max_attempts = 10\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"\\nAttempt {attempt + 1}/{max_attempts}\")\n",
    "        result = scrape_dex_data(proxy_rotator)\n",
    "\n",
    "        if result:\n",
    "            print(\"Successfully extracted content:\")\n",
    "            print(result)\n",
    "            # Add delay between requests\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "        else:\n",
    "            print(\"Failed to extract content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 of 3\n",
      "Request failed with status code: 403\n",
      "Attempt 2 of 3\n",
      "Waiting 3.82 seconds before retry...\n",
      "Request failed with status code: 403\n",
      "Attempt 3 of 3\n",
      "Waiting 6.12 seconds before retry...\n",
      "Request failed with status code: 403\n",
      "\n",
      "Failed to extract content after all retries\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "\n",
    "def create_advanced_scraper():\n",
    "    # Custom headers that make the request look more like a real browser\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "    }\n",
    "\n",
    "    # Create scraper with custom settings\n",
    "    scraper = cloudscraper.create_scraper(\n",
    "        browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"desktop\": True},\n",
    "        delay=10,  # Delay for solving challenges\n",
    "    )\n",
    "\n",
    "    # Update headers\n",
    "    scraper.headers.update(headers)\n",
    "\n",
    "    return scraper\n",
    "\n",
    "\n",
    "def scrape_with_retries(\n",
    "    url,\n",
    "    max_retries=3,\n",
    "    selector=\"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\",\n",
    "):\n",
    "    scraper = create_advanced_scraper()\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1} of {max_retries}\")\n",
    "\n",
    "            # Add random delay between attempts\n",
    "            if attempt > 0:\n",
    "                delay = random.uniform(3, 7)\n",
    "                print(f\"Waiting {delay:.2f} seconds before retry...\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "            # Make the request\n",
    "            response = scraper.get(url)\n",
    "\n",
    "            # Check if request was successful\n",
    "            if response.status_code == 200:\n",
    "                print(\"Successfully retrieved page\")\n",
    "\n",
    "                # Parse the content\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                target_element = soup.select_one(selector)\n",
    "\n",
    "                if target_element:\n",
    "                    return target_element.text\n",
    "                else:\n",
    "                    print(\"Element not found in the page content\")\n",
    "\n",
    "                    # Debug: Print page title to verify we're getting the right page\n",
    "                    page_title = soup.title.text if soup.title else \"No title found\"\n",
    "                    print(f\"Page title: {page_title}\")\n",
    "\n",
    "                    # If it's the last attempt, save the HTML for debugging\n",
    "                    if attempt == max_retries - 1:\n",
    "                        with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(response.text)\n",
    "                        print(\"Saved HTML content to 'debug_page.html' for inspection\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "        except cloudscraper.exceptions.CloudflareChallengeError as e:\n",
    "            print(f\"Cloudflare challenge error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        url = \"https://dexscreener.com/\"\n",
    "        result = scrape_with_retries(url)\n",
    "\n",
    "        if result:\n",
    "            print(\"\\nExtracted content:\")\n",
    "            print(result)\n",
    "        else:\n",
    "            print(\"\\nFailed to extract content after all retries\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nScript failed with error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Proxies:\n",
      "47.251.122.81:8888\n",
      "72.10.160.94:25353\n",
      "13.38.153.36:80\n",
      "13.37.59.99:3128\n",
      "13.38.176.104:3128\n",
      "13.36.104.85:80\n",
      "13.36.113.81:3128\n",
      "13.36.87.105:3128\n",
      "15.236.106.236:3128\n",
      "3.127.62.252:80\n",
      "52.67.10.183:80\n",
      "8.219.102.193:2000\n",
      "47.239.217.242:80\n",
      "37.187.25.85:80\n",
      "54.67.125.45:3128\n",
      "184.169.154.119:80\n",
      "54.204.67.108:56551\n",
      "3.136.29.104:80\n",
      "200.174.198.86:8888\n",
      "35.72.118.126:80\n",
      "35.76.62.196:80\n",
      "18.228.149.161:80\n",
      "18.185.169.150:3128\n",
      "46.51.249.135:3128\n",
      "54.233.119.172:3128\n",
      "52.196.1.182:80\n",
      "204.236.176.61:3128\n",
      "35.247.176.243:8080\n",
      "3.141.217.225:80\n",
      "13.59.156.167:3128\n",
      "113.160.132.195:8080\n",
      "54.248.238.110:80\n",
      "128.53.168.21:8080\n",
      "204.236.137.68:80\n",
      "44.219.175.186:80\n",
      "51.255.57.241:80\n",
      "134.209.23.180:8888\n",
      "91.92.96.210:8080\n",
      "47.236.231.113:8888\n",
      "194.4.57.200:3128\n",
      "34.219.161.232:3128\n",
      "43.200.108.126:3128\n",
      "44.195.247.145:80\n",
      "13.56.192.187:80\n",
      "3.90.100.12:80\n",
      "72.10.160.173:7003\n",
      "13.37.73.214:80\n",
      "44.218.183.55:80\n",
      "3.71.239.218:3128\n",
      "3.122.84.99:3128\n",
      "3.139.242.184:80\n",
      "43.200.77.128:3128\n",
      "43.201.121.81:80\n",
      "3.12.144.146:3128\n",
      "3.129.184.210:80\n",
      "54.152.3.36:80\n",
      "63.35.64.177:3128\n",
      "216.229.112.25:8080\n",
      "43.202.154.212:80\n",
      "3.37.125.76:3128\n",
      "3.212.148.199:3128\n",
      "13.208.56.180:80\n",
      "67.43.227.230:18389\n",
      "13.37.89.201:80\n",
      "27.79.141.7:16000\n",
      "222.252.194.204:8080\n",
      "39.61.54.80:8080\n",
      "187.141.125.210:8080\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Path to your WebDriver\n",
    "webdriver_path = \"/path/to/chromedriver\"  # Replace with your WebDriver path\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(\"\")\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # Open the website\n",
    "    driver.get(\"https://free-proxy-list.net/\")\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Locate the proxy table using the provided selector\n",
    "    table = driver.find_element(\n",
    "        By.CSS_SELECTOR, \"#list > div > div.table-responsive > div > table\"\n",
    "    )\n",
    "\n",
    "    # Extract the table rows (excluding the header row)\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Skip the header row\n",
    "\n",
    "    # Parse the data and collect proxies\n",
    "    proxies = []\n",
    "    for row in rows:\n",
    "        cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(cols) >= 8:\n",
    "            ip = cols[0].text\n",
    "            port = cols[1].text\n",
    "            https = cols[6].text\n",
    "            if https.lower() == \"yes\":  # Include only HTTPS proxies\n",
    "                proxies.append(f\"{ip}:{port}\")\n",
    "\n",
    "    # Print the extracted proxies\n",
    "    print(\"Extracted Proxies:\")\n",
    "    for proxy in proxies:\n",
    "        print(proxy)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched Proxies: \n",
      "\n",
      " 47.251.122.81:8888\n",
      "\t72.10.160.94:25353\n",
      "\t72.10.160.172:31575\n",
      "\t44.195.247.145:80\n",
      "\t93.113.63.73:33100\n",
      "\t54.67.125.45:3128\n",
      "\t184.169.154.119:80\n",
      "\t13.56.192.187:80\n",
      "\t3.136.29.104:80\n",
      "\t3.122.84.99:3128\n",
      "\t3.124.133.93:3128\n",
      "\t18.185.169.150:3128\n",
      "\t204.236.176.61:3128\n",
      "\t13.59.156.167:3128\n",
      "\t113.160.132.195:8080\n",
      "\t3.90.100.12:80\n",
      "\t27.79.141.7:16000\n",
      "\t206.189.41.13:8888\n",
      "\t52.73.224.54:3128\n",
      "\t44.219.175.186:80\n",
      "\t51.255.57.241:80\n",
      "\t15.236.106.236:3128\n",
      "\t44.218.183.55:80\n",
      "\t37.187.25.85:80\n",
      "\t54.204.67.108:56551\n",
      "\t200.174.198.86:8888\n",
      "\t43.202.154.212:80\n",
      "\t3.37.125.76:3128\n",
      "\t18.228.149.161:80\n",
      "\t43.200.77.128:3128\n",
      "\t43.201.121.81:80\n",
      "\t52.67.10.183:80\n",
      "\t54.152.3.36:80\n",
      "\t128.53.168.21:8080\n",
      "\t134.209.23.180:8888\n",
      "\t13.38.153.36:80\n",
      "\t13.37.59.99:3128\n",
      "\t13.38.176.104:3128\n",
      "\t13.36.104.85:80\n",
      "\t13.36.113.81:3128\n",
      "\t13.36.87.105:3128\n",
      "\t3.127.62.252:80\n",
      "\t8.219.102.193:2000\n",
      "\t47.239.217.242:80\n",
      "\t35.72.118.126:80\n",
      "\t35.76.62.196:80\n",
      "\t46.51.249.135:3128\n",
      "\t54.233.119.172:3128\n",
      "\t52.196.1.182:80\n",
      "\t35.247.176.243:8080\n",
      "\t3.141.217.225:80\n",
      "\t54.248.238.110:80\n",
      "\t204.236.137.68:80\n",
      "\t91.92.96.210:8080\n",
      "\t47.236.231.113:8888\n",
      "\t194.4.57.200:3128\n",
      "\t34.219.161.232:3128\n",
      "\t43.200.108.126:3128\n",
      "\t72.10.160.173:7003\n",
      "\t13.37.73.214:80\n",
      "\t3.71.239.218:3128\n",
      "\t3.139.242.184:80\n",
      "\t3.12.144.146:3128\n",
      "\t3.129.184.210:80\n",
      "\t63.35.64.177:3128\n",
      "\t216.229.112.25:8080\n",
      "\t3.212.148.199:3128\n",
      "\t13.208.56.180:80\n",
      "\t67.43.227.230:18389\n",
      "\t13.37.89.201:80\n",
      "\t222.252.194.204:8080\n",
      "--------------------------------------------------\n",
      "Found 71 proxies.\n",
      "Attempt 1 of 3\n",
      "Using proxy: 200.174.198.86:8888\n",
      "Unexpected error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy. Your proxy appears to only use HTTP and not HTTPS, try changing your proxy URL to be HTTP. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#https-proxy-error-http-proxy', SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:1000)'))))\n",
      "Attempt 2 of 3\n",
      "Using proxy: 13.208.56.180:80\n",
      "Waiting 5.01 seconds before retry...\n",
      "Unexpected error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n",
      "Attempt 3 of 3\n",
      "Using proxy: 3.124.133.93:3128\n",
      "Waiting 6.89 seconds before retry...\n",
      "Unexpected error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n",
      "\n",
      "Script failed with error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))))\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Path to your WebDriver\n",
    "webdriver_path = \"/path/to/chromedriver\"  # Replace with your WebDriver path\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(\"\")\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "\n",
    "def extract_proxies():\n",
    "    # Open the website to get the proxy list\n",
    "    driver.get(\"https://free-proxy-list.net/\")\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Locate the proxy table using the provided selector\n",
    "    table = driver.find_element(\n",
    "        By.CSS_SELECTOR, \"#list > div > div.table-responsive > div > table\"\n",
    "    )\n",
    "\n",
    "    # Extract the table rows (excluding the header row)\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Skip the header row\n",
    "\n",
    "    # Parse the data and collect proxies\n",
    "    proxies = []\n",
    "    for row in rows:\n",
    "        cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(cols) >= 8:\n",
    "            ip = cols[0].text\n",
    "            port = cols[1].text\n",
    "            https = cols[6].text\n",
    "            if https.lower() == \"yes\":  # Include only HTTPS proxies\n",
    "                proxies.append(f\"{ip}:{port}\")\n",
    "\n",
    "    print(f\"Fetched Proxies: \\n\\n {\"\\n\\t\".join(proxies)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return proxies\n",
    "\n",
    "\n",
    "def create_advanced_scraper():\n",
    "    # Custom headers that make the request look more like a real browser\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "    }\n",
    "\n",
    "    # Create scraper with custom settings\n",
    "    scraper = cloudscraper.create_scraper(\n",
    "        browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"desktop\": True},\n",
    "        delay=10,  # Delay for solving challenges\n",
    "    )\n",
    "\n",
    "    # Update headers\n",
    "    scraper.headers.update(headers)\n",
    "\n",
    "    return scraper\n",
    "\n",
    "\n",
    "def scrape_with_retries(\n",
    "    url,\n",
    "    proxies,\n",
    "    max_retries=3,\n",
    "    selector=\"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\",\n",
    "):\n",
    "    scraper = create_advanced_scraper()\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1} of {max_retries}\")\n",
    "\n",
    "            # Pick a random proxy from the list\n",
    "            proxy = random.choice(proxies)\n",
    "            print(f\"Using proxy: {proxy}\")\n",
    "\n",
    "            # Set the proxy for the scraper\n",
    "            scraper.proxies = {\"http\": f\"http://{proxy}\", \"https\": f\"https://{proxy}\"}\n",
    "\n",
    "            # Add random delay between attempts\n",
    "            if attempt > 0:\n",
    "                delay = random.uniform(3, 7)\n",
    "                print(f\"Waiting {delay:.2f} seconds before retry...\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "            # Make the request\n",
    "            response = scraper.get(url)\n",
    "\n",
    "            # Check if request was successful\n",
    "            if response.status_code == 200:\n",
    "                print(\"Successfully retrieved page\")\n",
    "\n",
    "                # Parse the content\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                target_element = soup.select_one(selector)\n",
    "\n",
    "                if target_element:\n",
    "                    return target_element.text\n",
    "                else:\n",
    "                    print(\"Element not found in the page content\")\n",
    "\n",
    "                    # Debug: Print page title to verify we're getting the right page\n",
    "                    page_title = soup.title.text if soup.title else \"No title found\"\n",
    "                    print(f\"Page title: {page_title}\")\n",
    "\n",
    "                    # If it's the last attempt, save the HTML for debugging\n",
    "                    if attempt == max_retries - 1:\n",
    "                        with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(response.text)\n",
    "                        print(\"Saved HTML content to 'debug_page.html' for inspection\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "        except cloudscraper.exceptions.CloudflareChallengeError as e:\n",
    "            print(f\"Cloudflare challenge error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Extract proxies using Selenium\n",
    "        proxies = extract_proxies()\n",
    "        print(f\"Found {len(proxies)} proxies.\")\n",
    "\n",
    "        if proxies:\n",
    "            url = \"https://dexscreener.com/\"\n",
    "            result = scrape_with_retries(url, proxies)\n",
    "\n",
    "            if result:\n",
    "                print(\"\\nExtracted content:\")\n",
    "                print(result)\n",
    "            else:\n",
    "                print(\"\\nFailed to extract content after all retries\")\n",
    "        else:\n",
    "            print(\"No proxies found!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nScript failed with error: {str(e)}\")\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched Proxies: \n",
      "\n",
      " 129.146.163.153:47060\n",
      "\t134.209.23.180:8888\n",
      "\t43.200.108.126:3128\n",
      "\t8.219.97.248:80\n",
      "\t13.38.153.36:80\n",
      "\t13.37.59.99:3128\n",
      "\t13.38.176.104:3128\n",
      "\t13.36.104.85:80\n",
      "\t13.36.113.81:3128\n",
      "\t13.36.87.105:3128\n",
      "\t13.37.73.214:80\n",
      "\t44.218.183.55:80\n",
      "\t44.195.247.145:80\n",
      "\t37.187.25.85:80\n",
      "\t54.67.125.45:3128\n",
      "\t184.169.154.119:80\n",
      "\t13.56.192.187:80\n",
      "\t54.204.67.108:56551\n",
      "\t3.136.29.104:80\n",
      "\t13.208.56.180:80\n",
      "\t3.126.147.182:80\n",
      "\t3.71.239.218:3128\n",
      "\t35.72.118.126:80\n",
      "\t35.76.62.196:80\n",
      "\t35.79.120.242:3128\n",
      "\t3.127.62.252:80\n",
      "\t3.124.133.93:3128\n",
      "\t18.228.149.161:80\n",
      "\t18.185.169.150:3128\n",
      "\t3.127.121.101:80\n",
      "\t3.123.150.192:80\n",
      "\t46.51.249.135:3128\n",
      "\t3.139.242.184:80\n",
      "\t3.12.144.146:3128\n",
      "\t18.228.198.164:80\n",
      "\t3.78.92.159:3128\n",
      "\t3.129.184.210:80\n",
      "\t3.212.148.199:3128\n",
      "\t3.141.217.225:80\n",
      "\t13.59.156.167:3128\n",
      "\t113.160.132.195:8080\n",
      "\t3.90.100.12:80\n",
      "\t128.53.168.21:8080\n",
      "\t52.73.224.54:3128\n",
      "\t44.219.175.186:80\n",
      "\t222.252.194.204:8080\n",
      "\t47.251.122.81:8888\n",
      "\t72.10.160.94:25353\n",
      "\t72.10.160.172:31575\n",
      "\t93.113.63.73:33100\n",
      "\t3.122.84.99:3128\n",
      "\t204.236.176.61:3128\n",
      "\t27.79.141.7:16000\n",
      "\t206.189.41.13:8888\n",
      "\t51.255.57.241:80\n",
      "\t200.174.198.86:8888\n",
      "\t43.202.154.212:80\n",
      "\t3.37.125.76:3128\n",
      "\t43.200.77.128:3128\n",
      "\t43.201.121.81:80\n",
      "\t52.67.10.183:80\n",
      "\t54.152.3.36:80\n",
      "\t8.219.102.193:2000\n",
      "\t47.239.217.242:80\n",
      "\t54.233.119.172:3128\n",
      "\t52.196.1.182:80\n",
      "\t35.247.176.243:8080\n",
      "\t54.248.238.110:80\n",
      "\t204.236.137.68:80\n",
      "\t91.92.96.210:8080\n",
      "\t47.236.231.113:8888\n",
      "\t194.4.57.200:3128\n",
      "\t34.219.161.232:3128\n",
      "\t72.10.160.173:7003\n",
      "\t63.35.64.177:3128\n",
      "\t216.229.112.25:8080\n",
      "--------------------------------------------------\n",
      "Found 76 proxies.\n",
      "Attempt 1 of 3\n",
      "Using proxy: 35.72.118.126:80\n",
      "Unexpected error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:2559)')))\n",
      "Attempt 2 of 3\n",
      "Using proxy: 54.67.125.45:3128\n",
      "Waiting 5.77 seconds before retry...\n",
      "Unexpected error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:2559)')))\n",
      "Attempt 3 of 3\n",
      "Using proxy: 52.196.1.182:80\n",
      "Waiting 3.03 seconds before retry...\n",
      "Unexpected error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:2559)')))\n",
      "\n",
      "Script failed with error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:2559)')))\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import urllib3\n",
    "import ssl\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(\"\")\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def extract_proxies():\n",
    "    driver.get(\"https://free-proxy-list.net/\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    table = driver.find_element(\n",
    "        By.CSS_SELECTOR, \"#list > div > div.table-responsive > div > table\"\n",
    "    )\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")[1:]\n",
    "    \n",
    "    proxies = []\n",
    "    for row in rows:\n",
    "        cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(cols) >= 8:\n",
    "            ip = cols[0].text\n",
    "            port = cols[1].text\n",
    "            https = cols[6].text\n",
    "            if https.lower() == \"yes\":\n",
    "                proxies.append(f\"{ip}:{port}\")\n",
    "    \n",
    "    print(f\"Fetched Proxies: \\n\\n {'\\n\\t'.join(proxies)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return proxies\n",
    "\n",
    "def create_advanced_scraper():\n",
    "    # Create SSL context that doesn't verify certificates\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    }\n",
    "    \n",
    "    scraper = cloudscraper.create_scraper(\n",
    "        browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"desktop\": True},\n",
    "        delay=10,\n",
    "        ssl_context=ssl_context  # Use our custom SSL context\n",
    "    )\n",
    "    \n",
    "    scraper.headers.update(headers)\n",
    "    return scraper\n",
    "\n",
    "def scrape_with_retries(url, proxies, max_retries=3, selector=\"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\"):\n",
    "    scraper = create_advanced_scraper()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1} of {max_retries}\")\n",
    "            proxy = random.choice(proxies)\n",
    "            print(f\"Using proxy: {proxy}\")\n",
    "            \n",
    "            proxies_dict = {\n",
    "                \"http\": f\"http://{proxy}\",\n",
    "                \"https\": f\"https://{proxy}\"\n",
    "            }\n",
    "            \n",
    "            if attempt > 0:\n",
    "                delay = random.uniform(3, 7)\n",
    "                print(f\"Waiting {delay:.2f} seconds before retry...\")\n",
    "                time.sleep(delay)\n",
    "            \n",
    "            # Use the proxies dictionary and don't verify SSL\n",
    "            response = scraper.get(url, proxies=proxies_dict, verify=False)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                print(\"Successfully retrieved page\")\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                target_element = soup.select_one(selector)\n",
    "                \n",
    "                if target_element:\n",
    "                    return target_element.text\n",
    "                else:\n",
    "                    print(\"Element not found\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(response.text)\n",
    "                        print(\"Saved HTML content for inspection\")\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "                \n",
    "        except cloudscraper.exceptions.CloudflareChallengeError as e:\n",
    "            print(f\"Cloudflare challenge error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        proxies = extract_proxies()\n",
    "        print(f\"Found {len(proxies)} proxies.\")\n",
    "        \n",
    "        if proxies:\n",
    "            url = \"https://dexscreener.com/\"\n",
    "            result = scrape_with_retries(url, proxies)\n",
    "            \n",
    "            if result:\n",
    "                print(\"\\nExtracted content:\")\n",
    "                print(result)\n",
    "            else:\n",
    "                print(\"\\nFailed to extract content after all retries\")\n",
    "        else:\n",
    "            print(\"No proxies found!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nScript failed with error: {str(e)}\")\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched Proxies: \n",
      "\n",
      " 129.146.163.153:47060\n",
      "\t134.209.23.180:8888\n",
      "\t43.200.108.126:3128\n",
      "\t8.219.97.248:80\n",
      "\t13.38.153.36:80\n",
      "\t13.37.59.99:3128\n",
      "\t13.38.176.104:3128\n",
      "\t13.36.104.85:80\n",
      "\t13.36.113.81:3128\n",
      "\t13.36.87.105:3128\n",
      "\t13.37.73.214:80\n",
      "\t44.218.183.55:80\n",
      "\t44.195.247.145:80\n",
      "\t37.187.25.85:80\n",
      "\t54.67.125.45:3128\n",
      "\t184.169.154.119:80\n",
      "\t13.56.192.187:80\n",
      "\t54.204.67.108:56551\n",
      "\t3.136.29.104:80\n",
      "\t13.208.56.180:80\n",
      "\t3.126.147.182:80\n",
      "\t3.71.239.218:3128\n",
      "\t35.72.118.126:80\n",
      "\t35.76.62.196:80\n",
      "\t35.79.120.242:3128\n",
      "\t3.127.62.252:80\n",
      "\t3.124.133.93:3128\n",
      "\t18.228.149.161:80\n",
      "\t18.185.169.150:3128\n",
      "\t3.127.121.101:80\n",
      "\t3.123.150.192:80\n",
      "\t46.51.249.135:3128\n",
      "\t3.139.242.184:80\n",
      "\t3.12.144.146:3128\n",
      "\t18.228.198.164:80\n",
      "\t3.78.92.159:3128\n",
      "\t3.129.184.210:80\n",
      "\t3.212.148.199:3128\n",
      "\t3.141.217.225:80\n",
      "\t13.59.156.167:3128\n",
      "\t113.160.132.195:8080\n",
      "\t3.90.100.12:80\n",
      "\t128.53.168.21:8080\n",
      "\t52.73.224.54:3128\n",
      "\t44.219.175.186:80\n",
      "\t222.252.194.204:8080\n",
      "\t47.251.122.81:8888\n",
      "\t72.10.160.94:25353\n",
      "\t72.10.160.172:31575\n",
      "\t93.113.63.73:33100\n",
      "\t3.122.84.99:3128\n",
      "\t204.236.176.61:3128\n",
      "\t27.79.141.7:16000\n",
      "\t206.189.41.13:8888\n",
      "\t51.255.57.241:80\n",
      "\t200.174.198.86:8888\n",
      "\t43.202.154.212:80\n",
      "\t3.37.125.76:3128\n",
      "\t43.200.77.128:3128\n",
      "\t43.201.121.81:80\n",
      "\t52.67.10.183:80\n",
      "\t54.152.3.36:80\n",
      "\t8.219.102.193:2000\n",
      "\t47.239.217.242:80\n",
      "\t54.233.119.172:3128\n",
      "\t52.196.1.182:80\n",
      "\t35.247.176.243:8080\n",
      "\t54.248.238.110:80\n",
      "\t204.236.137.68:80\n",
      "\t91.92.96.210:8080\n",
      "\t47.236.231.113:8888\n",
      "\t194.4.57.200:3128\n",
      "\t34.219.161.232:3128\n",
      "\t72.10.160.173:7003\n",
      "\t63.35.64.177:3128\n",
      "\t216.229.112.25:8080\n",
      "--------------------------------------------------\n",
      "Found 76 proxies.\n",
      "\n",
      "Script failed with error: Session.__init__() got an unexpected keyword argument 'session'\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "import urllib3\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Path to your WebDriver\n",
    "webdriver_path = \"/path/to/chromedriver\"  # Replace with your WebDriver path\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(\"\")\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Create an SSLContext to disable certificate verification\n",
    "http = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_NONE',  # Disable certificate validation\n",
    "    assert_hostname=False    # Disable hostname verification\n",
    ")\n",
    "\n",
    "def extract_proxies():\n",
    "    # Open the website to get the proxy list\n",
    "    driver.get(\"https://free-proxy-list.net/\")\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Locate the proxy table using the provided selector\n",
    "    table = driver.find_element(\n",
    "        By.CSS_SELECTOR, \"#list > div > div.table-responsive > div > table\"\n",
    "    )\n",
    "\n",
    "    # Extract the table rows (excluding the header row)\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Skip the header row\n",
    "\n",
    "    # Parse the data and collect proxies\n",
    "    proxies = []\n",
    "    for row in rows:\n",
    "        cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(cols) >= 8:\n",
    "            ip = cols[0].text\n",
    "            port = cols[1].text\n",
    "            https = cols[6].text\n",
    "            if https.lower() == \"yes\":  # Include only HTTPS proxies\n",
    "                proxies.append(f\"{ip}:{port}\")\n",
    "\n",
    "    print(f\"Fetched Proxies: \\n\\n {\"\\n\\t\".join(proxies)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return proxies\n",
    "\n",
    "def create_advanced_scraper():\n",
    "    # Custom headers that make the request look more like a real browser\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "    }\n",
    "\n",
    "    # Create a custom session and attach the urllib3 PoolManager\n",
    "    session = requests.Session()\n",
    "    adapter = HTTPAdapter(pool_connections=10, pool_maxsize=10)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    # Create cloudscraper with the session\n",
    "    scraper = cloudscraper.create_scraper(\n",
    "        browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"desktop\": True},\n",
    "        delay=10,  # Delay for solving challenges\n",
    "        session=session  # Attach the custom session to the scraper\n",
    "    )\n",
    "\n",
    "    # Update headers\n",
    "    scraper.headers.update(headers)\n",
    "\n",
    "    return scraper\n",
    "\n",
    "def scrape_with_retries(url, proxies, max_retries=3, selector=\"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\"):\n",
    "    scraper = create_advanced_scraper()\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1} of {max_retries}\")\n",
    "            proxy = random.choice(proxies)\n",
    "            print(f\"Using proxy: {proxy}\")\n",
    "            \n",
    "            scraper.proxies = {\"http\": f\"http://{proxy}\", \"https\": f\"https://{proxy}\"}\n",
    "\n",
    "            if attempt > 0:\n",
    "                delay = random.uniform(3, 7)\n",
    "                print(f\"Waiting {delay:.2f} seconds before retry...\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "            response = scraper.get(url)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                print(\"Successfully retrieved page\")\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                target_element = soup.select_one(selector)\n",
    "\n",
    "                if target_element:\n",
    "                    return target_element.text\n",
    "                else:\n",
    "                    print(\"Element not found\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(response.text)\n",
    "                        print(\"Saved HTML content for inspection\")\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "        except cloudscraper.exceptions.CloudflareChallengeError as e:\n",
    "            print(f\"Cloudflare challenge error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Extract proxies using Selenium\n",
    "        proxies = extract_proxies()\n",
    "        print(f\"Found {len(proxies)} proxies.\")\n",
    "\n",
    "        if proxies:\n",
    "            url = \"https://dexscreener.com/\"\n",
    "            result = scrape_with_retries(url, proxies)\n",
    "\n",
    "            if result:\n",
    "                print(\"\\nExtracted content:\")\n",
    "                print(result)\n",
    "            else:\n",
    "                print(\"\\nFailed to extract content after all retries\")\n",
    "        else:\n",
    "            print(\"No proxies found!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nScript failed with error: {str(e)}\")\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched Proxies: \n",
      "\n",
      " 129.146.163.153:47060\n",
      "\t134.209.23.180:8888\n",
      "\t43.200.108.126:3128\n",
      "\t8.219.97.248:80\n",
      "\t13.38.153.36:80\n",
      "\t13.37.59.99:3128\n",
      "\t13.38.176.104:3128\n",
      "\t13.36.104.85:80\n",
      "\t13.36.113.81:3128\n",
      "\t13.36.87.105:3128\n",
      "\t13.37.73.214:80\n",
      "\t44.218.183.55:80\n",
      "\t44.195.247.145:80\n",
      "\t37.187.25.85:80\n",
      "\t54.67.125.45:3128\n",
      "\t184.169.154.119:80\n",
      "\t13.56.192.187:80\n",
      "\t54.204.67.108:56551\n",
      "\t3.136.29.104:80\n",
      "\t13.208.56.180:80\n",
      "\t3.126.147.182:80\n",
      "\t3.71.239.218:3128\n",
      "\t35.72.118.126:80\n",
      "\t35.76.62.196:80\n",
      "\t35.79.120.242:3128\n",
      "\t3.127.62.252:80\n",
      "\t3.124.133.93:3128\n",
      "\t18.228.149.161:80\n",
      "\t18.185.169.150:3128\n",
      "\t3.127.121.101:80\n",
      "\t3.123.150.192:80\n",
      "\t46.51.249.135:3128\n",
      "\t3.139.242.184:80\n",
      "\t3.12.144.146:3128\n",
      "\t18.228.198.164:80\n",
      "\t3.78.92.159:3128\n",
      "\t3.129.184.210:80\n",
      "\t3.212.148.199:3128\n",
      "\t3.141.217.225:80\n",
      "\t13.59.156.167:3128\n",
      "\t113.160.132.195:8080\n",
      "\t3.90.100.12:80\n",
      "\t128.53.168.21:8080\n",
      "\t52.73.224.54:3128\n",
      "\t44.219.175.186:80\n",
      "\t222.252.194.204:8080\n",
      "\t47.251.122.81:8888\n",
      "\t72.10.160.94:25353\n",
      "\t72.10.160.172:31575\n",
      "\t93.113.63.73:33100\n",
      "\t3.122.84.99:3128\n",
      "\t204.236.176.61:3128\n",
      "\t27.79.141.7:16000\n",
      "\t206.189.41.13:8888\n",
      "\t51.255.57.241:80\n",
      "\t200.174.198.86:8888\n",
      "\t43.202.154.212:80\n",
      "\t3.37.125.76:3128\n",
      "\t43.200.77.128:3128\n",
      "\t43.201.121.81:80\n",
      "\t52.67.10.183:80\n",
      "\t54.152.3.36:80\n",
      "\t8.219.102.193:2000\n",
      "\t47.239.217.242:80\n",
      "\t54.233.119.172:3128\n",
      "\t52.196.1.182:80\n",
      "\t35.247.176.243:8080\n",
      "\t54.248.238.110:80\n",
      "\t204.236.137.68:80\n",
      "\t91.92.96.210:8080\n",
      "\t47.236.231.113:8888\n",
      "\t194.4.57.200:3128\n",
      "\t34.219.161.232:3128\n",
      "\t72.10.160.173:7003\n",
      "\t63.35.64.177:3128\n",
      "\t216.229.112.25:8080\n",
      "--------------------------------------------------\n",
      "Found 76 proxies.\n",
      "Attempt 1 of 3\n",
      "Using proxy: 3.129.184.210:80\n",
      "Unexpected error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:2559)')))\n",
      "Attempt 2 of 3\n",
      "Using proxy: 3.141.217.225:80\n",
      "Waiting 4.90 seconds before retry...\n",
      "Unexpected error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL] record layer failure (_ssl.c:2559)')))\n",
      "Attempt 3 of 3\n",
      "Using proxy: 72.10.160.172:31575\n",
      "Waiting 3.20 seconds before retry...\n",
      "Unexpected error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7568673b5fd0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n",
      "\n",
      "Script failed with error: HTTPSConnectionPool(host='dexscreener.com', port=443): Max retries exceeded with url: / (Caused by ProxyError('Unable to connect to proxy', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7568673b5fd0>: Failed to establish a new connection: [Errno 111] Connection refused')))\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import urllib3\n",
    "import ssl\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(\"\")  # Add the path to your ChromeDriver if needed\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def extract_proxies():\n",
    "    \"\"\"\n",
    "    Extract HTTPS-compatible proxies from free-proxy-list.net.\n",
    "    \"\"\"\n",
    "    driver.get(\"https://free-proxy-list.net/\")\n",
    "    time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "    # Locate the proxy table\n",
    "    table = driver.find_element(\n",
    "        By.CSS_SELECTOR, \"#list > div > div.table-responsive > div > table\"\n",
    "    )\n",
    "    rows = table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Skip the header row\n",
    "\n",
    "    proxies = []\n",
    "    for row in rows:\n",
    "        cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(cols) >= 8:\n",
    "            ip = cols[0].text\n",
    "            port = cols[1].text\n",
    "            https = cols[6].text\n",
    "            if https.lower() == \"yes\":  # Only use HTTPS-compatible proxies\n",
    "                proxies.append(f\"{ip}:{port}\")\n",
    "\n",
    "    print(f\"Fetched Proxies: \\n\\n {'\\n\\t'.join(proxies)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return proxies\n",
    "\n",
    "def create_advanced_scraper():\n",
    "    \"\"\"\n",
    "    Create a CloudScraper instance with custom SSL context and headers.\n",
    "    \"\"\"\n",
    "    # Create SSL context that doesn't verify certificates and uses modern TLS\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "    ssl_context.set_ciphers('DEFAULT@SECLEVEL=1')  # Lower security level to allow more ciphers\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    }\n",
    "\n",
    "    scraper = cloudscraper.create_scraper(\n",
    "        browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"desktop\": True},\n",
    "        delay=10,\n",
    "        ssl_context=ssl_context  # Use our custom SSL context\n",
    "    )\n",
    "\n",
    "    scraper.headers.update(headers)\n",
    "    return scraper\n",
    "\n",
    "def scrape_with_retries(url, proxies, max_retries=3, selector=\"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\"):\n",
    "    \"\"\"\n",
    "    Scrape a URL with retries and proxy rotation.\n",
    "    \"\"\"\n",
    "    scraper = create_advanced_scraper()\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1} of {max_retries}\")\n",
    "            proxy = random.choice(proxies)\n",
    "            print(f\"Using proxy: {proxy}\")\n",
    "\n",
    "            proxies_dict = {\n",
    "                \"http\": f\"http://{proxy}\",\n",
    "                \"https\": f\"https://{proxy}\"\n",
    "            }\n",
    "\n",
    "            if attempt > 0:\n",
    "                delay = random.uniform(3, 7)\n",
    "                print(f\"Waiting {delay:.2f} seconds before retry...\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "            # Use the proxies dictionary and don't verify SSL\n",
    "            response = scraper.get(url, proxies=proxies_dict, verify=False)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                print(\"Successfully retrieved page\")\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                target_element = soup.select_one(selector)\n",
    "\n",
    "                if target_element:\n",
    "                    return target_element.text\n",
    "                else:\n",
    "                    print(\"Element not found\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(response.text)\n",
    "                        print(\"Saved HTML content for inspection\")\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "        except cloudscraper.exceptions.CloudflareChallengeError as e:\n",
    "            print(f\"Cloudflare challenge error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Extract proxies\n",
    "        proxies = extract_proxies()\n",
    "        print(f\"Found {len(proxies)} proxies.\")\n",
    "\n",
    "        if proxies:\n",
    "            url = \"https://dexscreener.com/\"\n",
    "            result = scrape_with_retries(url, proxies)\n",
    "\n",
    "            if result:\n",
    "                print(\"\\nExtracted content:\")\n",
    "                print(result)\n",
    "            else:\n",
    "                print(\"\\nFailed to extract content after all retries\")\n",
    "        else:\n",
    "            print(\"No proxies found!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nScript failed with error: {str(e)}\")\n",
    "    finally:\n",
    "        # Clean up the WebDriver\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 of 3\n",
      "Error during attempt 1: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\"}\n",
      "  (Session info: chrome=131.0.6778.69); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "#0 0x582437c891fa <unknown>\n",
      "#1 0x582437799810 <unknown>\n",
      "#2 0x5824377e8506 <unknown>\n",
      "#3 0x5824377e87a1 <unknown>\n",
      "#4 0x58243782dc24 <unknown>\n",
      "#5 0x58243780c5ad <unknown>\n",
      "#6 0x58243782b007 <unknown>\n",
      "#7 0x58243780c323 <unknown>\n",
      "#8 0x5824377dade0 <unknown>\n",
      "#9 0x5824377dbdbe <unknown>\n",
      "#10 0x582437c5512b <unknown>\n",
      "#11 0x582437c590c7 <unknown>\n",
      "#12 0x582437c426cc <unknown>\n",
      "#13 0x582437c59c47 <unknown>\n",
      "#14 0x582437c2767f <unknown>\n",
      "#15 0x582437c78288 <unknown>\n",
      "#16 0x582437c78450 <unknown>\n",
      "#17 0x582437c88076 <unknown>\n",
      "#18 0x73376dc94ac3 <unknown>\n",
      "\n",
      "Waiting 6.29 seconds before retry...\n",
      "Attempt 2 of 3\n",
      "Error during attempt 2: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\"}\n",
      "  (Session info: chrome=131.0.6778.69); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "#0 0x582437c891fa <unknown>\n",
      "#1 0x582437799810 <unknown>\n",
      "#2 0x5824377e8506 <unknown>\n",
      "#3 0x5824377e87a1 <unknown>\n",
      "#4 0x58243782dc24 <unknown>\n",
      "#5 0x58243780c5ad <unknown>\n",
      "#6 0x58243782b007 <unknown>\n",
      "#7 0x58243780c323 <unknown>\n",
      "#8 0x5824377dade0 <unknown>\n",
      "#9 0x5824377dbdbe <unknown>\n",
      "#10 0x582437c5512b <unknown>\n",
      "#11 0x582437c590c7 <unknown>\n",
      "#12 0x582437c426cc <unknown>\n",
      "#13 0x582437c59c47 <unknown>\n",
      "#14 0x582437c2767f <unknown>\n",
      "#15 0x582437c78288 <unknown>\n",
      "#16 0x582437c78450 <unknown>\n",
      "#17 0x582437c88076 <unknown>\n",
      "#18 0x73376dc94ac3 <unknown>\n",
      "\n",
      "Waiting 3.56 seconds before retry...\n",
      "Attempt 3 of 3\n",
      "Error during attempt 3: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"#root > div > main > div > div.ds-dex-table.ds-dex-table-top\"}\n",
      "  (Session info: chrome=131.0.6778.69); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "#0 0x582437c891fa <unknown>\n",
      "#1 0x582437799810 <unknown>\n",
      "#2 0x5824377e8506 <unknown>\n",
      "#3 0x5824377e87a1 <unknown>\n",
      "#4 0x58243782dc24 <unknown>\n",
      "#5 0x58243780c5ad <unknown>\n",
      "#6 0x58243782b007 <unknown>\n",
      "#7 0x58243780c323 <unknown>\n",
      "#8 0x5824377dade0 <unknown>\n",
      "#9 0x5824377dbdbe <unknown>\n",
      "#10 0x582437c5512b <unknown>\n",
      "#11 0x582437c590c7 <unknown>\n",
      "#12 0x582437c426cc <unknown>\n",
      "#13 0x582437c59c47 <unknown>\n",
      "#14 0x582437c2767f <unknown>\n",
      "#15 0x582437c78288 <unknown>\n",
      "#16 0x582437c78450 <unknown>\n",
      "#17 0x582437c88076 <unknown>\n",
      "#18 0x73376dc94ac3 <unknown>\n",
      "\n",
      "Saved HTML content to 'debug_page.html' for inspection.\n",
      "\n",
      "Failed to extract content after all retries.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
